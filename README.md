# ETL Using PySpark on Google Dataproc

**How can we process a huge amount of data automatically without writing a script repeatedly?**

In this project, you have a huge amount of data from your client on your local computer stored in `data` folder. You should transform the data and store them into BigQuery as your Data Warehouse

## Project Overview

1. Move data from local storage to BigQuery

## Tech Stack

We're going to use some services from Google Cloud Platform here:

1. Google Cloud SDK
2. Google Dataproc
3. PySpark

## Data

Flight data, you can see the datasets on `data` folder

## Requirements

1. Google Cloud SDK. To set it up read it [here](https://cloud.google.com/sdk/docs/quickstart#installing_the_latest_version)
2. Git. For windows you can download git bash from [here](https://git-scm.com/downloads)
3. PySpark Knowledge. Read it [here](https://spark.apache.org/docs/latest/sql-getting-started.html)

## Installation

1. for linux run command `$ chmod +x bash_script.sh`
2. run `$ ./bash_script.h`